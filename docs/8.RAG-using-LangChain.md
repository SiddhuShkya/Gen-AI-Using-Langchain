# üìò RAG (Retrieval Augmented Generation) using LangChain

## üîπ What is a RAG?
RAG is a technique that combines **information retrieval** with **language generation**, where a model retrieves **relevant documents** from a knowledge base and then uses them as context to generate accurate and grounded responses.

Benefits of using RAG :
- Use of up-to-date information
- Better Privacy
- No limit of Document size

---

## üîπ What are the main components of an RAG applications?
There are 4 main components that are required to build an RAG based appllication:

- ‚úÖ **Document Loaders** ‚Üí Used to ingest raw data from different sources (PDFs, websites, databases, APIs, etc.) into the system.
- ‚úÖ **Text Splitters** ‚Üí Break long documents into smaller, manageable chunks so embeddings and retrieval work effectively.
- ‚úÖ **Vector Databases** ‚Üí Store and index embeddings of text chunks for fast similarity search and retrieval.
- ‚úÖ **Retrievers** ‚Üí Query the vector database to fetch the most relevant chunks of information for a given user query.

---

## üîπ Document Loaders
Document Loaders are the components responsible for getting your raw data into the framework so it can be processed (chunking, embedding, retrieval and generation). They act as data ingestion tools that read content from different sources and return it in a standard format (Document objects) which contain two main parts:

- **`Page Content`** ‚Üí The actual text data  
- **`Metadata`** ‚Üí Useful extra info (e.g., source name, file path, author, date)  

There are 4 main document loaders that are widely used in langchain and they are:

---

### - **`1.TextLoader`** 

> It is an simple and commonly used document loader in langchain that reads plain text (.txt) file and converts them into a Langchain document objects. It is ideal for loading chat logs, scraped text, transcripts, code snippets, or any plain text data into langchain pipeline. However it only works with .txt files.

```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader('filename.txt', encoding='utf-8')
docs = loader.load()
print(type(docs))
```

---

### - **`2.PyPDFLoader`** 

> PyPDFLoader is a document loader in LangChain used to load content from **PDF files** and convert each poge into a Document object.

```python
[
    Document(page_content="Text from page 1", metadata={'page': 0, 'source': 'file.pdf'}),
    Document(page_content="Text from page 2", metadata={'page': 1, 'source': 'file.pdf'})
]
```

It uses the **PyPDF library** under the hood - not great with scanned PDFs or complex layouts.

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("file.pdf")

docs = loader.load()
print(docs)
```

Types of PDF Loader : 

| Use Case                        | Recommended Loader        | 
|---------------------------------|-----------------|
| Simple, clean PDFs              | PyPDFLoader       | 
| PDFs with tables/Columns        | PDFPlumberLoader | 
| Scanned/image PDFs              | UsnstructuredPDFLoader or AmazonTextractPDFLoader   | 
| Need layout and image data      | PyMuPDFLoader             | 
| Want best structure extraction  | UnstructuredPDFLoader            | 

---

### - **`3.DirectoryLoader`** 

> DirectoryLoader is a document loader that lets you load multiple documents from a directory (folder) or files.

| Glob Pattern                        | What it loads        | 
|---------------------------------|-----------------|
| **/*.txt              | All .txt files in all subfolders       | 
| *.pdf       | All .pdf files in the root directory | 
| data/*csv              | All .csv files in the data/ folder   | 
| **/*   | All files (any type/ all folders)             | 
| **  | recursive search through subfolders            | 

> Code Example :

```python
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader

loader = DirectoryLoader(
    path="../data/Books",
    glob="*.pdf",
    loader_cls=PyPDFLoader,
)

docs = loader.load()
```

---

### - **`4.WebBaseLoader`** 

> WebBaseLoader is a document loader in LangChain used to load and extract text content from web pages (URLs).

> It uses Requests and BeautifulSoup under the hood to parse HTML and extract visible text.

- **`When to Use`** ‚Üí For blogs, news articles, or public websites where the content is primarily text-based and static.

- **`Limitations`**

    ‚Üí Doesn't handle JavaScript-heavy pages well (use SeleniumURLLoader for that).

    ‚Üí Loads only static component (what's in the HTML, not what loads after the page renders)

```python
from langchain_community.document_loaders import WebBaseLoader

url = "https://httpbin.org/html"

loader = WebBaseLoader(url)
docs = loader.load()

content = docs[0].page_content
print(content)

```

---

### - **`5.CSVLoader`** 

> It is a document loader used to load CSV files into LangChain Document objects open per row, by default.

```python
from langchain_community.document_loaders import CSVLoader

loader = CSVLoader(file_path="data.csv")

docs = loader.load()
print(f"First document content: \n{docs[0].page_content}") 
print(f"Metadata of first document: {docs[0].metadata}")
print(f"Loaded {len(docs)} document(s).")
```

> ‚ö†Ô∏è **Important Notes on Loaders** :  

1. There are additinally more document loaders in langchain for various tasks other thanm the above 5. To learn about them click [here](https://python.langchain.com/docs/integrations/document_loaders/).

2. You can also create your own document loader by following steps in the below link.

    üëâ [link](https://python.langchain.com/docs/how_to/document_loader_custom/#standard-document-loader)

3. Every loaders have two methods *load()* and *lazy_laod()* 

| load()                          | lazy_load()        | 
|---------------------------------|-----------------|
| Eager Loading (loads everything at once) | Lazy Loading (loads on demand)       | 
| Returns: A list of Document objects       | Returns: A generator of Document objects | 
| Loads all documents immediately into memory              | Documents are not all loaded at once: they're fetched one at a time as needed   | 
| Best when : number of docs is small, want to load everything upfront   | Best when : dealing with large number of documents/files, stream processing (e.g chunking, embedding)        | 

---

## üîπ Text Splitters