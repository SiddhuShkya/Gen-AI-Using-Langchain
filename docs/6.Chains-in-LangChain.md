# ðŸ“‘ Chains in LangChain

## ðŸ”¹ What Are Chains?
When building applications with Large Language Models (LLMs), a single prompt â†’ response flow is often not enough.  
Applications usually require **multiple steps** that connect together.  

A **Chain** in LangChain is a way to **combine multiple components**â€”such as prompts, LLMs, output parsers, and moreâ€”into a single, reusable workflow. Basically chains in langchain helps to create a pipeline that helps us to control the workflow of our LLM based application using using minimal coding.

---

## ðŸ”¹ Why Chains?
- **Structured Workflows**: Instead of writing messy, step-by-step code, chains allow you to build modular pipelines.  
- **Reusability**: Define once, reuse across applications.  
- **Composability**: Easily combine prompts, models, tools, and parsers.  
- **Debuggability**: Inspect each step clearly.  

**Example:**  
A simple app may need:  
1. Take a question from a user.  
2. Format it into a prompt.  
3. Send it to the LLM.  
4. Parse and return the answer.  

This can be defined as a **Chain**.

---

## ðŸ”¹ Types of Chains
LangChain provides multiple kinds of chains:

1. **Simple Chains**  
   - A single LLM call with a prompt.  
   - Example: `LLMChain`

2. **Sequential Chains**  
   - Output of one step is passed to the next.  
   - Example: `SimpleSequentialChain`, `SequentialChain`

3. **Router Chains**  
   - Dynamically choose which chain to run based on input.  
   - Example: `MultiPromptChain`

4. **Custom Chains**  
   - You can build your own chain logic by extending the `Chain` class.

---

## ðŸ”¹ Example: Simple Chain

```python
from dotenv import load_dotenv 
from langchain_core.prompts import PromptTemplate 
from langchain_core.output_parsers import StrOutputParser 
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint

load_dotenv()

llm = HuggingFaceEndpoint(
   repo_id="google/gemma-2-2b-it",
   task="text-generation",
   huggingfacehub_api_token=api_token,
   timeout=120
)

model = ChatHuggingFace(llm=llm)

prompt = PromptTemplate(
    template='Generate 5 interesting facts about {topic}',
    input_variables=['topic']
)

parser = StrOutputParser()
chain = prompt | model | parser 
result = chain.invoke({'topic': 'Large Language Models'})
print(result)

