# ðŸ“˜ Runnables in LangChain

## ðŸ”¹ What are Runnables?
In **LangChain**, a **Runnable** is the **fundamental unit of work**.  
It represents any executable component that follows a **common interface**.

Every Runnable exposes a consistent set of methods like:
- **`invoke`** â†’ Run a single input synchronously.  
- **`batch`** â†’ Run multiple inputs at once.  
- **`stream`** â†’ Run and receive results incrementally (streaming).  

Because of this shared interface, different Runnables can be **connected together like LEGO blocks** to form flexible and complex workflows.  

---

## ðŸ”¹ Why Runnables?
- âœ… **Consistency** â†’ Every component (LLMs, prompts, chains, tools) can be used in the same way.  
- âœ… **Composability** â†’ Easily combine different pieces into workflows.  
- âœ… **Scalability** â†’ Handle single calls, batch processing, or streaming seamlessly.  
- âœ… **Flexibility** â†’ Works with any backend (OpenAI, HuggingFace, Anthropic, etc.) without changing the logic.  

ðŸ‘‰ In short: **Runnables make LangChain pipelines modular, reusable, and predictable.**

---

## ðŸ”¹ How Runnables Work
You can think of Runnables as a **contract** â€” if a class implements `invoke`, `batch`, and `stream`, you can slot it anywhere in your LangChain workflow.

```python
# A Runnable exposes the following methods:
runnable.invoke(input)   # Process a single input
runnable.batch(inputs)   # Process multiple inputs
runnable.stream(input)   # Process with streaming output
```

---

## ðŸ”¹ Types of Runables

> 1. Task Specific Runnables

- **Definition** â†’ Core LangChain components converted into runnables so they can be used in pipelines.  
- **Purpose** â†’ Perform task-specific operations such as LLM calls, computation, or retrieval.  
- **Examples**:  
  - `ChatOpenAI` â†’ Runs an LLM model.  
  - `Retriever` â†’ Retrieves relevant documents.  
  - `PromptTemplate` â†’ Formats inputs into structured prompts.
    
> 2. Runnable Primitives

- **Definition** â†’ These are fundamental building blocks for structuring execution logic in AI workflows.
- **Purpose** â†’ They help orchestrate execution by defining how different Runnables interact (sequentially, in parallel, conditionally etc).
- **Examples**:  
  - `RunnableSequence` â†’ Runs steps in order ( | operator) 
  - `RunnableParallel` â†’ Runs multiple steps simultaneously.  
  - `RunnableMap` â†’ Maps the same input across multiple functions.
  - `RunnableBranch` â†’ Implements conditional execution into Runnables 
  - `RunnableLambda` â†’ Wraps custom Python functions into Runnables.  
  - `RunnablePassthrough` â†’ Just forwards input as output (acts as a placehoder).